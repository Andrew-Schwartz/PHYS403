{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb36f68b",
   "metadata": {},
   "source": [
    "# Homework 6 - Andrew Schwartz - PHYS 403, SPR 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a566fb5",
   "metadata": {},
   "source": [
    "## 1. Model Selection for Bayesians: the Bayes Factor\n",
    "\n",
    "In Bayesian model selection, the probability of a model $M_i$ given data $D$ is\n",
    "\n",
    "$$p(M_i|D)=\\frac{p(D|M_i)p(M_i)}{p(D)}$$ \n",
    "\n",
    "where \n",
    "\n",
    "$$p(D|M_i)=\\int_\\vec\\theta p(D|\\vec\\theta)p(\\vec\\theta|M_i)d\\vec\\theta,\\quad\\quad p(D)=\\sum_{M_i} p(D|M_i)p(M_i),$$ \n",
    "\n",
    "and the vector $\\vec\\theta={\\theta_1,\\theta_2,...}$ represents the parameters of model $M_i$. In the likelihood $p(D|M_i)$ we marginalize over the prior distributions of the model parameters. The “evidence” $p(D)$ is calculated by summing over all models of interest. When choosing between two models $M_i$ and $M_j$, we calculate the posterior odds \n",
    "\n",
    "$$\\frac{p(M_i|D)}{p(M_j|D)}=\\frac{p(D|M_i)}{p(D|M_j)}\\frac{p(M_i)}{p(M_j)}=B_{ij}\\frac{p(M_i)}{p(M_j)}.$$\n",
    "\n",
    "Assuming no reason to favor model $M_i$ over $M_j$ before taking data, the posterior odds ratio reduces to the Bayes factor $B_{ij}$. Calculating $p(D)$ is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01a1ee",
   "metadata": {},
   "source": [
    "### (a) \n",
    "\n",
    "Consider the $xy$ data plotted below, which you can access in the file model $\\texttt{selection.txt}$ on Blackboard.\n",
    "\n",
    "![Pretend that this is the image of the data](data6.png)\n",
    "\n",
    "Let $M_1$ correspond to a linear model with two parameters $\\vec\\theta = {a, b}$, $$y = a + bx.$$ Assuming flat priors on $a$ and $b$, calculate $p(D|M_1)$ if the uncertainties on the data are Gaussian. When (numerically) integrating the likelihood, you can assume that the range of $a$ and $b$ is between -2 and 2."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# just getting stuff set up\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import nquad\n",
    "\n",
    "xs, ys, dys = np.fromstring(\"\"\"0.02    -0.26     0.10 \n",
    "    0.03    -0.25     0.10 \n",
    "    0.05    -0.09     0.10 \n",
    "    0.13    -0.18     0.10 \n",
    "    0.21    -0.06     0.10 \n",
    "    0.26    -0.03     0.10 \n",
    "    0.28     0.28     0.10 \n",
    "    0.29    -0.03     0.10 \n",
    "    0.42     0.11     0.10 \n",
    "    0.44     0.22     0.10 \n",
    "    0.46     0.23     0.10 \n",
    "    0.51     0.29     0.10 \n",
    "    0.55     0.45     0.10 \n",
    "    0.56     0.12     0.10 \n",
    "    0.59     0.33     0.10 \n",
    "    0.65     0.52     0.10 \n",
    "    0.68     0.35     0.10 \n",
    "    0.71     0.40     0.10 \n",
    "    0.89     0.84     0.10 \n",
    "    0.90     0.72     0.10\"\"\", dtype=float, sep=' ').reshape((20, 3)).T\n",
    "\n",
    "# dy = dy[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:21:01.102467Z",
     "start_time": "2024-04-03T17:21:01.088341Z"
    }
   },
   "id": "466f0b019fb7fe36",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "p(D|M_i)&=\\int_\\vec\\theta p(D|\\vec\\theta)p(\\vec\\theta|M_i)d\\vec\\theta \\\\\n",
    "&=\\int_{-2}^2 d a \\int_{-2}^2 db\\ p(D|\\vec\\theta)p(\\vec\\theta|M_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $$ \n",
    "p(D|\\vec\\theta)=p(D|a,b)=\\prod_{i=1}^np(y_i|x_i,a,b)=\\left(\\frac{1}{2\\pi\\delta_y^2}\\right)^{\\frac{n}{2}}\\exp\\left(-\\frac{(\\vec{y}-(a+b \\vec x))^2}{2\\delta_y^2}\\right)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87cb52dd4d920360"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "30255519.71725677"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prob_linear(x, y, dy, a, b):\n",
    "    dif = (y - (a + b * x)) / dy\n",
    "    num = np.dot(dif, dif)\n",
    "    norm = np.prod(1 / np.sqrt(2 * np.pi * dys * dys))\n",
    "    return norm * np.exp(-num / 2)\n",
    "\n",
    "def prior(nargs):\n",
    "    return 1 / 4 ** nargs\n",
    "\n",
    "prob_linear(xs, ys, dys, -0.26, 1.06)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T18:37:00.849225Z",
     "start_time": "2024-04-03T18:37:00.821238Z"
    }
   },
   "id": "2492af94ed443435",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.20000000000000004"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:34:07.313551Z",
     "start_time": "2024-04-03T17:34:07.295667Z"
    }
   },
   "id": "55d057b14537d42a",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "baf94773",
   "metadata": {},
   "source": [
    "### (b) \n",
    "\n",
    "Let $M_2$ correspond to a quadratic model with three parameters $\\vec\\theta = {a, b, c}$, $$y = a + bx + cx^2.$$ Calculate $p(D|M_2)$, using the same priors on $a$ and $b$ and a uniform prior on $c$ (which you may also assume has a range of -2 to 2)."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6b643f86c26d4f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (c)\n",
    "\n",
    "Compute the Bayes Factor $B_{12}$. Do you favor model $M_1$ or the more complex model $M_2$? Justify your answer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90205bade6db03e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "_3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85219e9ec99357d7"
  },
  {
   "cell_type": "markdown",
   "id": "d6e8e9b1",
   "metadata": {},
   "source": [
    "## 2. Model Selection for Frequentists: Wilks’ Theorem\n",
    "\n",
    "Repeat the model selection of the previous problem, but this time use a frequentist approach on the data in model $\\texttt{selection.txt}$ to determine if the linear or quadratic model is a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (a)\n",
    "\n",
    "Consider a model $M_1$ in which the data are described by the linear relationship $$y = a + bx.$$ Minimize the chi-square test statistic for this linear model, where the test statistic is given by the sum of the square of the residuals $$\\chi^2_{M_1}=\\sum_{i=1}^N\\left(\\frac{y_i-(a+b x_i)}{\\sigma_i}\\right)^2.$$\n",
    "\n",
    "Report $\\chi^2_{M_1}$ and the best estimates of $\\hat{a}$ and $\\hat b$. Plot the best fit line on top of the data.\n",
    "\n",
    "Using the fact that $\\chi^2_{M_1}$ is asymptotically distributed like $\\chi^2_{N−2}$ (a chi-square with $N − 2$ degrees of freedom) calculate the $p$-value that the data are a random fluctuation from the linear model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "212c8e608c3757fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (b)\n",
    "\n",
    "Repeat part (a) for the quadratic model $M_2$, where $$y = a + bx + cx^2.$$ \n",
    "\n",
    "I.e., minimize and report $\\chi^2_{M_2}$ and the best estimates $\\hat a$, $\\hat b$, and $\\hat c$, and plot the best fit line on top of the data. \n",
    "\n",
    "Using the fact that $\\chi^2_{M_2}$ is asymptotically distributed like $\\chi^2_{N−3}$ – a chi-square with $N − 3$ degrees of freedom – calculate the $p$-value that the data are a random fluctuation from the quadratic model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a69af0c9ecfa90fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (c)\n",
    "\n",
    "Models $M_1$ and $M_2$ are *nested*: that is, the parameters of $M_1$ are a subset of\n",
    "$M_2$. Therefore, the chi-square difference $\\Delta\\chi^2 = \\chi^2_{M_1}− \\chi^2_{M_2}$ should be distributed like $\\chi^2_{3−2} = \\chi^2_1$. I.e., $$\\Delta\\chi^2 ∼ \\chi^2_1,$$\n",
    "where the number of degrees of freedom is equal to the difference between the number of parameters in the two models (3 − 2 = 1). This result is known as **Wilks’ Theorem**.\n",
    "\n",
    "A large chi-square difference implies that the difference between the two models fits is\n",
    "statistically significant. Calculate $\\Delta\\chi^2$ for models $M_1$ and $M_2$ and compute its $p$-value. Is the difference between the models statistically significant? Justify your answer given what you know about the pitfalls of model selection with $p$-values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a53289bb98d4057c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:19:55.256596Z",
     "start_time": "2024-03-26T21:19:55.249232Z"
    }
   },
   "id": "77c5151a93817681",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "79afcc8d8bbe46c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
